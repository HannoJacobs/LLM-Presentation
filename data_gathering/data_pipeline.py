#!/usr/bin/env python3
"""
Consolidated Data Pipeline for Animal Wikipedia Collection

This single file automatically creates 3 dataset sizes with direct article content from your animal list:
- Scrapes all animals once (respectful to Wikipedia)
- Truncates each article to first MAX_WORDS_PER_ARTICLE words
- Nano: First 5 animals from list
- Mini: First 10 animals from list
- Full: All animals from list
- Content: Direct article text only

Just run: python3 data_pipeline.py
"""

import wikipediaapi
import json
import os
import re
from datetime import datetime
from pathlib import Path
import time
import random
import sys

# Global base path for the project - automatically determined from script location
# This gets the parent directory of the data_gathering folder (i.e., the project root)
BASE_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# Relative path constants
DATASETS_DIR = "Datasets"
PROCESSED_DIR = os.path.join(DATASETS_DIR, "processed")
SRC_DIR = "src"
MODELS_DIR = "models"

ANIMAL_LIST = [
    # Top 5 animals for fine-tuning (keep these)
    "Cat",
    "Dog",
    "Elephant",
    "Lion",
    "Tiger",
    # ===== BASIC CONCEPTS & FUNDAMENTALS =====
    "Color",
    "Shape",
    "Number",
    "Time",
    "Day",
    "Night",
    "Week",
    "Month",
    "Year",
    "Season",
    "Clock",
    "Calendar",
    "Mathematics",
    "Measurement",
    "Weight",
    "Length",
    "Volume",
    "Temperature",
    # ===== NATURAL ELEMENTS & PHENOMENA =====
    "Water",
    "Fire",
    "Earth",
    "Air",
    "Light",
    "Sound",
    "Heat",
    "Cold",
    "Weather",
    "Rain",
    "Snow",
    "Wind",
    "Storm",
    "Cloud",
    "Sky",
    "Ground",
    "Soil",
    "Rock",
    "Metal",
    "Wood",
    "Glass",
    "Stone",
    # ===== SPACE & ASTRONOMY =====
    "Sun",
    "Moon",
    "Star",
    "Planet",
    "Galaxy",
    "Universe",
    "Solar System",
    "Earth",
    "Mars",
    "Jupiter",
    "Saturn",
    "Mercury",
    "Venus",
    "Uranus",
    "Neptune",
    "Comet",
    "Asteroid",
    "Meteor",
    "Black Hole",
    "Supernova",
    # ===== GEOGRAPHY & NATURE =====
    "Ocean",
    "Sea",
    "Lake",
    "River",
    "Mountain",
    "Valley",
    "Hill",
    "Plain",
    "Desert",
    "Forest",
    "Jungle",
    "Island",
    "Continent",
    "Country",
    "City",
    "Town",
    "Village",
    "Road",
    "Bridge",
    "Beach",
    "Coast",
    "Volcano",
    "Earthquake",
    # ===== BIOLOGY & LIFE =====
    "Plant",
    "Tree",
    "Flower",
    "Leaf",
    "Root",
    "Seed",
    "Fruit",
    "Vegetable",
    "Grass",
    "Animal",
    "Bird",
    "Fish",
    "Insect",
    "Mammal",
    "Reptile",
    "Amphibian",
    "Life",
    "Death",
    "Birth",
    "Growth",
    "Evolution",
    # ===== HUMAN BODY & HEALTH =====
    "Body",
    "Head",
    "Face",
    "Eye",
    "Ear",
    "Nose",
    "Mouth",
    "Hand",
    "Foot",
    "Heart",
    "Brain",
    "Blood",
    "Bone",
    "Muscle",
    "Skin",
    "Hair",
    "Breath",
    "Sleep",
    "Dream",
    "Health",
    "Disease",
    "Medicine",
    "Doctor",
    "Hospital",
    # ===== FOOD & NUTRITION =====
    "Food",
    "Water",
    "Milk",
    "Bread",
    "Meat",
    "Fish",
    "Egg",
    "Rice",
    "Wheat",
    "Sugar",
    "Salt",
    "Oil",
    "Butter",
    "Cheese",
    "Fruit",
    "Apple",
    "Orange",
    "Banana",
    "Grape",
    "Lemon",
    "Potato",
    "Tomato",
    "Carrot",
    "Onion",
    "Cooking",
    "Baking",
    "Eating",
    "Hunger",
    "Thirst",
    # ===== ARTS & CULTURE =====
    "Art",
    "Music",
    "Painting",
    "Drawing",
    "Sculpture",
    "Dance",
    "Theater",
    "Film",
    "Book",
    "Story",
    "Poetry",
    "Novel",
    "Play",
    "Song",
    "Singer",
    "Actor",
    "Artist",
    "Writer",
    "Poet",
    "Musician",
    "Instrument",
    "Piano",
    "Guitar",
    "Drum",
    "Violin",
    # ===== LITERATURE & LANGUAGE =====
    "Language",
    "Word",
    "Letter",
    "Sentence",
    "Story",
    "Book",
    "Library",
    "Writing",
    "Reading",
    "Speaking",
    "Listening",
    "Grammar",
    "Dictionary",
    "Newspaper",
    "Magazine",
    "Letter",
    "Email",
    "Telephone",
    # ===== HISTORY & SOCIETY =====
    "History",
    "Past",
    "Present",
    "Future",
    "Ancient",
    "Modern",
    "War",
    "Peace",
    "King",
    "Queen",
    "President",
    "Government",
    "Law",
    "Court",
    "Police",
    "Army",
    "Religion",
    "God",
    "Church",
    "Temple",
    "Prayer",
    "Holiday",
    "Celebration",
    "Festival",
    # ===== SCIENCE & TECHNOLOGY =====
    "Science",
    "Physics",
    "Chemistry",
    "Biology",
    "Mathematics",
    "Computer",
    "Machine",
    "Engine",
    "Electricity",
    "Energy",
    "Power",
    "Technology",
    "Internet",
    "Telephone",
    "Television",
    "Radio",
    "Camera",
    "Clock",
    "Watch",
    "Car",
    "Airplane",
    "Ship",
    "Train",
    "Bicycle",
    "Robot",
    "Factory",
    # ===== SOCIAL & EMOTIONAL =====
    "Family",
    "Parent",
    "Child",
    "Brother",
    "Sister",
    "Friend",
    "Love",
    "Hate",
    "Happy",
    "Sad",
    "Angry",
    "Fear",
    "Joy",
    "Pain",
    "Hope",
    "Dream",
    "Wish",
    "Memory",
    "Thought",
    "Feeling",
    "Emotion",
    "Mind",
    "Soul",
    "Spirit",
    # ===== WORK & EDUCATION =====
    "Work",
    "Job",
    "Teacher",
    "Student",
    "School",
    "University",
    "Class",
    "Lesson",
    "Test",
    "Exam",
    "Grade",
    "Degree",
    "Office",
    "Business",
    "Company",
    "Money",
    "Bank",
    "Shop",
    "Market",
    "Store",
    "Buy",
    "Sell",
    "Price",
    "Cost",
    # ===== GAMES & SPORTS =====
    "Game",
    "Sport",
    "Football",
    "Soccer",
    "Basketball",
    "Baseball",
    "Tennis",
    "Golf",
    "Swimming",
    "Running",
    "Jumping",
    "Chess",
    "Card",
    "Dice",
    "Board Game",
    "Video Game",
    "Play",
    "Fun",
    "Win",
    "Lose",
    "Team",
    "Player",
    "Coach",
    "Champion",
    # ===== HOUSE & HOME =====
    "House",
    "Home",
    "Room",
    "Door",
    "Window",
    "Wall",
    "Floor",
    "Ceiling",
    "Bed",
    "Chair",
    "Table",
    "Kitchen",
    "Bathroom",
    "Garden",
    "Street",
    "Building",
    "Apartment",
    "Hotel",
    "Restaurant",
    "Park",
    # ===== CLOTHING & FASHION =====
    "Clothing",
    "Shirt",
    "Pants",
    "Dress",
    "Shoe",
    "Hat",
    "Coat",
    "Jacket",
    "Sock",
    "Glove",
    "Belt",
    "Watch",
    "Jewelry",
    "Ring",
    "Necklace",
    "Gold",
    "Silver",
    # ===== COLORS & QUALITIES =====
    "Red",
    "Blue",
    "Green",
    "Yellow",
    "Black",
    "White",
    "Big",
    "Small",
    "Long",
    "Short",
    "Tall",
    "Wide",
    "Narrow",
    "Fast",
    "Slow",
    "Hot",
    "Cold",
    "Hard",
    "Soft",
    "Heavy",
    "Light",
    "Clean",
    "Dirty",
    "New",
    "Old",
    "Beautiful",
    "Ugly",
    "Good",
    "Bad",
    "Right",
    "Wrong",
    "True",
    "False",
    # ===== ABSTRACT CONCEPTS =====
    "Freedom",
    "Justice",
    "Truth",
    "Beauty",
    "Knowledge",
    "Wisdom",
    "Power",
    "Strength",
    "Weakness",
    "Courage",
    "Fear",
    "Success",
    "Failure",
    "Beginning",
    "End",
    "Change",
    "Same",
    "Different",
    "Important",
    "Simple",
    "Complex",
]

# Dataset size configurations
DATASET_CONFIG = {
    "nano": 5,  # First 5 animals from list
    "mini": 10,  # First 10 animals from list
    "full": len(ANIMAL_LIST),  # All animals from list
}

# Configuration for content processing
MAX_WORDS_PER_ARTICLE = 10000  # Maximum words per article (configurable)
BINARY_CHECK_CHARS = 10000  # Number of characters to check for binary content

os.makedirs(os.path.join(BASE_PATH, DATASETS_DIR), exist_ok=True)


class AnimalWikiScraper:
    """Wikipedia scraper for animal articles"""

    def __init__(self, datasets_dir=os.path.join(BASE_PATH, DATASETS_DIR)):
        self.wiki = wikipediaapi.Wikipedia(
            user_agent="AnimalDatasetScraper/1.0 (Educational LLM Training Project)",
            language="en",
        )
        self.datasets_dir = datasets_dir
        self.ensure_datasets_dir()

    def ensure_datasets_dir(self):
        """Ensure the datasets directory exists"""
        if not os.path.exists(self.datasets_dir):
            os.makedirs(self.datasets_dir)

    def get_animal_list(self, size="full"):
        """Get animal list from global configuration with size-based selection"""
        # Return subset based on size
        if size == "nano":
            count = min(DATASET_CONFIG["nano"], len(ANIMAL_LIST))
            return ANIMAL_LIST[:count]
        elif size == "mini":
            count = min(DATASET_CONFIG["mini"], len(ANIMAL_LIST))
            return ANIMAL_LIST[:count]
        else:  # "full"
            return ANIMAL_LIST

    def scrape_animal_page(self, animal_name):
        """Scrape a single animal's Wikipedia page"""
        try:
            page = self.wiki.page(animal_name)

            if not page.exists():
                print(f"‚ùå Page for {animal_name} does not exist")
                return None

            # Get the full text content
            content = page.text

            # Extract basic info
            data = {
                "animal_name": animal_name,
                "title": page.title,
                "url": page.fullurl,
                "content": content,
                "summary": page.summary[:500] if page.summary else "",
                "content_length": len(content),
                "scraped_at": datetime.now().isoformat(),
            }

            return data

        except Exception as e:
            print(f"‚ùå Error scraping {animal_name}: {str(e)}")
            return None

    def save_daily_dataset(self, scraped_data, date_str=None):
        """Save the scraped data to a daily JSON file"""
        if date_str is None:
            date_str = datetime.now().strftime("%Y-%m-%d")

        filename = f"animal_wiki_dataset_{date_str}.json"
        filepath = os.path.join(self.datasets_dir, filename)

        # Filter out None values (failed scrapes)
        valid_data = [item for item in scraped_data if item is not None]

        dataset = {
            "metadata": {
                "date_created": date_str,
                "total_animals": len(valid_data),
                "scraper_version": "1.0",
            },
            "animals": valid_data,
        }

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(dataset, f, indent=2, ensure_ascii=False)

        print(f"üíæ Saved {len(valid_data)} animal articles to {filepath}")
        return filepath

    def scrape_daily_animals(self, size="full", delay_range=(1, 3)):
        """Scrape a set of animal Wikipedia pages based on dataset size"""
        animals = self.get_animal_list(size=size)

        scraped_data = []
        successful = 0

        size_name = {
            "nano": "Nano",
            "mini": "Mini",
            "full": "Full",
        }[size]
        print(f"üöÄ Starting to scrape {size_name}...")

        for i, animal in enumerate(animals, 1):
            print(f"üìÑ Scraping {i}/{len(animals)}: {animal}")

            data = self.scrape_animal_page(animal)
            if data:
                scraped_data.append(data)
                successful += 1

            # Add random delay to be respectful to Wikipedia
            if i < len(animals):  # Don't delay after the last request
                delay = random.uniform(*delay_range)
                time.sleep(delay)

        print(f"‚úÖ Successfully scraped {successful}/{len(animals)} animals")

        # Save the dataset
        filepath = self.save_daily_dataset(scraped_data)

        return scraped_data, filepath


class AnimalDataProcessor:
    """Process scraped data for LLM training"""

    def __init__(self, datasets_dir=os.path.join(BASE_PATH, DATASETS_DIR)):
        self.datasets_dir = Path(datasets_dir)
        self.processed_dir = self.datasets_dir / os.path.basename(PROCESSED_DIR)
        self.processed_dir.mkdir(exist_ok=True)

    def load_dataset(self, dataset_path):
        """Load a scraped dataset from JSON file"""
        with open(dataset_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def clean_text(self, text):
        """Clean text for LLM training"""
        if not text:
            return ""

        # Remove excessive whitespace
        text = re.sub(r"\n+", "\n", text)
        text = re.sub(r" +", " ", text)

        # Remove Wikipedia-specific formatting
        text = re.sub(r"\[\d+\]", "", text)  # Remove citation numbers [1], [2], etc.
        text = re.sub(r"==.*?==", "", text)  # Remove section headers
        text = re.sub(r"^\s*$", "", text, flags=re.MULTILINE)  # Remove empty lines

        # Truncate to first MAX_WORDS_PER_ARTICLE words
        words = text.split()
        if len(words) > MAX_WORDS_PER_ARTICLE:
            text = " ".join(words[:MAX_WORDS_PER_ARTICLE])

        return text.strip()

    def process_animal_data(self, animal_data):
        """Process a single animal's data for training"""
        content = self.clean_text(animal_data["content"])

        return {
            "animal_name": animal_data["animal_name"],
            "title": animal_data["title"],
            "url": animal_data["url"],
            "content": content,
            "content_length": len(content),
            "scraped_at": animal_data["scraped_at"],
        }

    def create_combined_training_data(self, size="full"):
        """Create a combined training file from all processed datasets with different sizes"""
        processed_files = list(
            self.processed_dir.glob("processed_animal_dataset_*.json")
        )

        if not processed_files:
            print("No processed dataset files found. Run process_all_datasets() first.")
            return None

        all_training_texts = []
        total_animals = 0

        for processed_file in sorted(processed_files):
            with open(processed_file, "r", encoding="utf-8") as f:
                dataset = json.load(f)

            for animal in dataset["animals"]:
                # Format: Animal name followed by content
                formatted_content = f"{animal['animal_name']}\n\n{animal['content']}"
                all_training_texts.append(formatted_content)
                total_animals += 1

        # Apply size filtering
        if size == "nano":
            # First 5 animals
            all_training_texts = all_training_texts[:5]
            total_animals = min(5, total_animals)
        elif size == "mini":
            # Half of all animals
            half_size = len(all_training_texts) // 2
            all_training_texts = all_training_texts[:half_size]
            total_animals = half_size
        # "full" uses all animals (default)

        # Save combined training file
        size_suffix = f"_{size}"
        combined_filename = f"animal_data{size_suffix}.txt"
        combined_path = self.processed_dir / combined_filename

        with open(combined_path, "w", encoding="utf-8") as f:
            for i, text in enumerate(all_training_texts):
                f.write(text)
                if i < len(all_training_texts) - 1:
                    f.write("\n\n\n")

        size_name = {
            "nano": "Nano (5 animals)",
            "mini": "Mini (50% animals)",
            "full": "Full (all animals)",
        }[size]
        print(f"üì¶ Created {size_name} training file: {combined_path}")
        print(f"   üìä Total training samples: {total_animals}")

        # Calculate statistics
        total_chars = sum(len(text) for text in all_training_texts)
        avg_chars = total_chars / total_animals if total_animals > 0 else 0

        print(f"   üìè Total characters: {total_chars:,}")
        print(f"Average characters per sample: {avg_chars:.2f}")
        return combined_path

    def process_all_datasets(self):
        """Process all dataset files in the datasets directory"""
        dataset_files = list(self.datasets_dir.glob("animal_wiki_dataset_*.json"))

        if not dataset_files:
            print("‚ùå No dataset files found in the datasets directory.")
            return []

        processed_files = []
        for dataset_file in sorted(dataset_files):
            print(f"üîÑ Processing dataset: {dataset_file}")
            processed_file = self.process_dataset_file(dataset_file)
            processed_files.append(processed_file)

        print(f"\n‚úÖ Processed {len(processed_files)} dataset files.")
        return processed_files

    def process_dataset_file(self, dataset_path):
        """Process an entire dataset file"""
        dataset = self.load_dataset(dataset_path)
        processed_animals = []

        for animal in dataset["animals"]:
            processed = self.process_animal_data(animal)
            processed_animals.append(processed)

        # Save processed data
        date_str = dataset["metadata"]["date_created"]
        output_filename = f"processed_animal_dataset_{date_str}.json"
        output_path = self.processed_dir / output_filename

        processed_dataset = {
            "metadata": {
                "original_dataset": os.path.basename(dataset_path),
                "date_created": date_str,
                "total_animals": len(processed_animals),
                "processing_date": datetime.now().isoformat(),
                "processor_version": "1.0",
            },
            "animals": processed_animals,
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(processed_dataset, f, indent=2, ensure_ascii=False)

        print(
            f"üíæ Processed {len(processed_animals)} animals and saved to {output_path}"
        )
        return output_path


class DataValidator:
    """Validate training data format and statistics"""

    def __init__(self, datasets_dir=os.path.join(BASE_PATH, DATASETS_DIR)):
        self.datasets_dir = Path(datasets_dir)
        self.processed_dir = self.datasets_dir / os.path.basename(PROCESSED_DIR)

    def verify_text_format(self):
        """Verify all training files are in plain text format"""

        if not self.processed_dir.exists():
            print("‚ùå Processed directory not found")
            return False

        # Find all training text files
        txt_files = list(self.processed_dir.glob("*.txt"))

        if not txt_files:
            print("‚ùå No training text files found")
            return False

        # Group files by size and format
        file_groups = {"nano": [], "mini": [], "full": [], "individual": []}

        for txt_file in txt_files:
            filename = txt_file.name
            if "_nano_" in filename:
                file_groups["nano"].append(txt_file)
            elif "_mini_" in filename:
                file_groups["mini"].append(txt_file)
            elif (
                "combined_" in filename
                and "_nano_" not in filename
                and "_mini_" not in filename
            ):
                file_groups["full"].append(txt_file)
            else:
                file_groups["individual"].append(txt_file)

        print("üìä Dataset Overview:")
        print(f"   Total files: {len(txt_files)}")
        print(f"   üß¨ Nano (5 animals): {len(file_groups['nano'])} files")
        print(f"   üìä Mini (50% animals): {len(file_groups['mini'])} files")
        print(f"   üåç Full (all animals): {len(file_groups['full'])} files")
        print(f"   üìÑ Individual files: {len(file_groups['individual'])} files")
        print("=" * 50)

        all_valid = True

        for txt_file in sorted(txt_files):
            print(f"\nüìÑ {txt_file.name}")

            try:
                # Read the file and check if it's valid text
                with open(txt_file, "r", encoding="utf-8") as f:
                    content = f.read()

                # Basic checks
                if len(content) == 0:
                    print("‚ùå File is empty")
                    all_valid = False
                    continue

                # Check for binary characters (very basic check)
                binary_chars = 0
                for char in content[:BINARY_CHECK_CHARS]:  # Check first N chars
                    if ord(char) < 32 and char not in "\n\r\t":
                        binary_chars += 1

                if binary_chars > 0:
                    print(
                        f"‚ö†Ô∏è  Found {binary_chars} potential binary characters in first {BINARY_CHECK_CHARS} chars"
                    )

                # Show sample content
                lines = content.split("\n")
                print(f"‚úì Total lines: {len(lines)}")
                print(f"‚úì Total characters: {len(content):,}")
                print(f"‚úì File size: {txt_file.stat().st_size:,} bytes")

                # Show first few lines as sample
                sample_lines = lines[:5]
                print("üìù Sample content:")
                for i, line in enumerate(sample_lines, 1):
                    if line.strip():  # Only show non-empty lines
                        print(f"   {i}: {line[:100]}{'...' if len(line) > 100 else ''}")

            except Exception as e:
                print(f"‚ùå Error reading file: {e}")
                all_valid = False

        print("\n" + "=" * 50)
        if all_valid:
            print("‚úÖ All training files are valid plain text format!")
            print("‚úÖ Ready for LLM training")
        else:
            print("‚ùå Some files have issues - please check above")

        return all_valid


def run_data_pipeline():
    """Run the complete data collection and processing pipeline for all 3 sizes"""

    print("=" * 60)
    print(
        f"üöÄ Starting Animal Data Collection Pipeline - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    )
    print("=" * 60)

    # Step 1: Scrape all animals once
    print("\n1Ô∏è‚É£ üìä SCRAPING PHASE")
    print("üîÑ Scraping all animals...")

    scraper = AnimalWikiScraper()
    scraped_data, dataset_path = scraper.scrape_daily_animals(size="full")

    if not scraped_data:
        print("‚ùå Failed to scrape animals")
        return False

    print(f"‚úÖ Successfully scraped {len(scraped_data)} animals")
    print("üìä Creating dataset sizes from single scrape...")

    # Step 2: Process the data for LLM training
    print("\n2Ô∏è‚É£ üîÑ PROCESSING PHASE")
    processor = AnimalDataProcessor()

    print("üîÑ Processing complete dataset...")
    processed_path = processor.process_dataset_file(dataset_path)
    print(f"‚úÖ Processed data: {os.path.basename(processed_path)}")

    # Create size-specific subsets from the processed data
    print("üìä Creating size-specific datasets...")

    # Step 3: Create training files for all sizes
    print("\n3Ô∏è‚É£ üì¶ TRAINING DATA GENERATION")

    training_files = []
    for size, size_count in DATASET_CONFIG.items():
        print(f"\nüîÑ Creating {size.upper()} dataset ({size_count} animals)...")

        try:
            combined_file = processor.create_combined_training_data(size=size)
            if combined_file:
                training_files.append(combined_file)
                print(f"    ‚úì Article content: {os.path.basename(combined_file)}")
        except Exception as e:
            print(f"    ‚úó Failed for {size}: {e}")

        print(f"‚úÖ {size.upper()} dataset complete")

    print(f"‚úÖ All training files generated: {len(training_files)} files")
    print("üí° All created from single scrape - efficient and Wikipedia-friendly!")

    # Step 4: Validation
    print("\n4Ô∏è‚É£ üîç VALIDATION PHASE")
    validator = DataValidator()
    validation_success = validator.verify_text_format()

    # Step 5: Summary
    print("\n5Ô∏è‚É£ üìä PIPELINE SUMMARY")
    print("=" * 60)
    print("‚úÖ Pipeline completed successfully!")
    print("=" * 60)
    print(f"üì¶ Training files created: {len(training_files)}")
    print("üìä Dataset breakdown:")
    print(f"   üß¨ Nano: 1 file ({DATASET_CONFIG['nano']} animals)")
    print(f"   üìä Mini: 1 file ({DATASET_CONFIG['mini']} animals)")
    print(f"   üåç Full: 1 file ({DATASET_CONFIG['full']} animals)")
    print(f"üìù Content: Direct article text ({MAX_WORDS_PER_ARTICLE} words max)")

    if validation_success:
        print("‚úÖ All validation checks passed!")
        print("üéØ Ready for LLM training!")
    else:
        print("‚ö†Ô∏è  Some validation checks failed - please review output above")

    return True


def main():
    """Main function - automatically runs the complete data collection pipeline"""

    print("üöÄ Starting automated data collection for all dataset sizes...")
    print(
        f"üìä Creating: Nano ({DATASET_CONFIG['nano']}), Mini ({DATASET_CONFIG['mini']}), Full ({DATASET_CONFIG['full']} animals)"
    )
    print(f"üìã Using {len(ANIMAL_LIST)} animals from configured list")

    # Run the complete pipeline (creates all sizes)
    success = run_data_pipeline()

    if success:
        print("\nüéâ Data collection completed successfully!")
        print("üìç Training data location: Datasets/processed/*.txt")
        print(
            f"üìä Created datasets: Nano ({DATASET_CONFIG['nano']}), Mini ({DATASET_CONFIG['mini']}), Full ({DATASET_CONFIG['full']} animals)"
        )
        print("üîß Ready for LLM training!")
    else:
        print("\n‚ùå Data collection failed!")
        sys.exit(1)


if __name__ == "__main__":
    main()
