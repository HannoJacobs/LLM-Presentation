# Consolidated Data Pipeline

This folder contains a single comprehensive script for collecting, processing, and managing animal Wikipedia data for LLM training.

## ğŸ“ Folder Structure

```
data_gathering/
â”œâ”€â”€ data_pipeline.py        # ğŸ”§ ALL-IN-ONE consolidated script
â””â”€â”€ README.md              # This documentation
```

## ğŸš€ Quick Start

### Run Complete Data Pipeline
```bash
cd data_gathering
python3 data_pipeline.py
```

That's it! The script automatically creates all 3 dataset sizes with no arguments needed.

## ğŸ“Š What the Pipeline Does

The single `data_pipeline.py` script handles everything efficiently:

1. **ğŸ“Š Single Scrape**: Collects all 30 animal Wikipedia articles once
2. **ğŸ”„ Processing**: Cleans and formats text for LLM training
3. **ğŸ“¦ Smart Generation**: Creates 3 dataset sizes from single scrape (no duplicates!)
4. **ğŸ” Validation**: Verifies plain text format and statistics
5. **ğŸ“‹ Results**: Ready-to-use training datasets

## ğŸ¯ Dataset Sizes Created

- **ğŸ§¬ Nano**: 5 animals (first 5 from curated list)
- **ğŸ“Š Mini**: 10 animals (first 10 from curated list)
- **ğŸŒ Full**: 30 animals (complete curated list)

## ğŸ“ Training Formats

Each dataset size includes 3 training formats:
- **Basic**: `Animal: Cat\n\n[Article content]`
- **With Summary**: Includes Wikipedia summary
- **Q&A**: `Question: Tell me about Cat.\nAnswer: [Content]`

## ğŸ“‚ Output Location

Training data is saved to:
```
/Datasets/processed/*.txt
```

**All files are in pure plain text format** ready for LLM training!

## ğŸ”§ Usage Examples

### Quick Test (Nano)
```bash
python3 data_pipeline.py --test
```

### Mini Dataset (10 animals)
```bash
python3 data_pipeline.py --size mini
```

### Full Dataset (30 animals)
```bash
python3 data_pipeline.py --size full
```

### View Usage Instructions
```bash
python3 data_pipeline.py --demo
```

## ğŸ“‹ Requirements

Make sure you have the required dependencies:
```bash
pip install wikipedia-api
```

## ğŸ“Š Expected Output

Recent test run creates:
- **9 training files** total
- **3 files per format** (nano, mini, full)
- **3 formats** (basic, with_summary, qa_format)
- **All in plain text** (.txt) format
- **~100K+ characters** per full dataset (30 animals)

## ğŸ¯ Use Cases

- **ğŸ§ª Testing**: Nano datasets for quick validation
- **ğŸ”¬ Research**: Mini datasets for experiments
- **ğŸ­ Production**: Full datasets for model training
- **ğŸ“š Education**: Complete pipeline for learning

## ğŸ”„ Automation

Set up cron jobs for automated data collection:
```bash
# Example: Run daily at 2 AM
0 2 * * * cd /path/to/data_gathering && python3 data_pipeline.py
```

## ğŸ“– Script Documentation

The `data_pipeline.py` file contains:

- `AnimalWikiScraper` class: Handles Wikipedia scraping
- `AnimalDataProcessor` class: Processes and formats data
- `DataValidator` class: Validates output format
- `run_data_pipeline()`: Main pipeline execution
- `show_usage_demo()`: Usage instructions
- `__main__`: Command-line interface

All functionality is consolidated into one maintainable file!
