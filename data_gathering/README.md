# Data Gathering Scripts

This folder contains all the scripts for collecting, processing, and managing animal Wikipedia data for LLM training.

## ğŸ“ Folder Structure

```
data_gathering/
â”œâ”€â”€ dataset_scraper.py      # Main Wikipedia scraping functionality
â”œâ”€â”€ data_processor.py       # Data processing and formatting for LLM training
â”œâ”€â”€ daily_scrape.py         # Automated daily data collection pipeline
â”œâ”€â”€ verify_text_format.py   # Verification script for plain text format
â”œâ”€â”€ training_data_demo.py   # Demo script showing how to use the data
â””â”€â”€ DATA_COLLECTION_README.md  # Detailed documentation
```

## ğŸš€ Quick Start

### Run Daily Data Collection
```bash
cd data_gathering
python3 daily_scrape.py --test    # Test mode (5 animals)
python3 daily_scrape.py           # Full collection (50 animals)
```

### Verify Data Format
```bash
cd data_gathering
python3 verify_text_format.py     # Check all training files
```

### View Usage Demo
```bash
cd data_gathering
python3 training_data_demo.py     # See how to use the data
```

## ğŸ“Š Dataset Sizes

The system creates 3 dataset sizes:
- **Nano**: 5 animals (quick testing)
- **Mini**: 50% of animals (development)
- **Full**: All animals (production)

Each size has 4 formats: Basic, With Summary, Q&A, Instruction.

## ğŸ“‚ Output Location

Training data is saved to:
```
/Datasets/processed/*.txt
```

All files are in **plain text format** ready for LLM training.

## ğŸ”§ Individual Scripts

### dataset_scraper.py
```python
from dataset_scraper import AnimalWikiScraper
scraper = AnimalWikiScraper()
scraper.scrape_daily_animals(max_animals=10)
```

### data_processor.py
```python
from data_processor import AnimalDataProcessor
processor = AnimalDataProcessor()
processor.process_all_datasets()
```

## ğŸ“‹ Requirements

Make sure you have the required dependencies:
```bash
pip install wikipedia-api
```

## ğŸ“– Documentation

See `DATA_COLLECTION_README.md` for comprehensive documentation including:
- Detailed usage examples
- Data format specifications
- Integration with LLM training pipelines
- Troubleshooting guide

## ğŸ¯ Use Cases

- **Research**: Academic studies on LLM training data
- **Development**: Testing and prototyping
- **Production**: Building animal knowledge bases
- **Education**: Teaching data collection and processing

## ğŸ”„ Automation

Set up cron jobs for automated daily data collection:
```bash
# Example: Run daily at 2 AM
0 2 * * * cd /path/to/data_gathering && python3 daily_scrape.py
```
