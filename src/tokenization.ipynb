{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63456a3",
   "metadata": {},
   "source": [
    "# Show how tokenization works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c55557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {' ': 0, 'a': 1, 'c': 2, 'e': 3, 'h': 4, 'm': 5, 'n': 6, 'o': 7, 's': 8, 't': 9}\n",
      "Tokens: ['t', 'h', 'e', ' ', 'c', 'a', 't', ' ', 's', 'a', 't', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'm', 'a', 't']\n",
      "Token IDs: [9, 4, 3, 0, 2, 1, 9, 0, 8, 1, 9, 0, 7, 6, 0, 9, 4, 3, 0, 5, 1, 9]\n",
      "length of vocab: 10\n",
      "length of tokens: 22\n"
     ]
    }
   ],
   "source": [
    "# Character-level tokenization\n",
    "text = \"the cat sat on the mat\"\n",
    "\n",
    "# Build vocabulary: unique characters mapped to IDs\n",
    "vocab = {ch: idx for idx, ch in enumerate(sorted(set(text)))}\n",
    "\n",
    "# Tokenize text into characters\n",
    "tokens = list(text)\n",
    "\n",
    "# Convert to IDs\n",
    "token_ids = [vocab[ch] for ch in tokens]\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(f\"length of vocab: {len(vocab)}\")\n",
    "print(f\"length of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79eb82ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'cat': 0, 'mat': 1, 'on': 2, 'sat': 3, 'the': 4}\n",
      "Tokens: ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "Token IDs: [4, 0, 3, 2, 4, 1]\n",
      "length of vocab: 5\n",
      "length of tokens: 6\n"
     ]
    }
   ],
   "source": [
    "# word-level tokenization\n",
    "text = \"the cat sat on the mat\"\n",
    "\n",
    "# Build vocabulary: unique words mapped to IDs\n",
    "tokens = text.split()\n",
    "vocab = {word: idx for idx, word in enumerate(sorted(set(tokens)))}\n",
    "\n",
    "# Convert to IDs\n",
    "token_ids = [vocab[word] for word in tokens]\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(f\"length of vocab: {len(vocab)}\")\n",
    "print(f\"length of tokens: {len(tokens)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
